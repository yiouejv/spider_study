Day06回顾
1、多线程爬虫
  1、多进程多线程应用场景
    1、多进程 ：大量密集并行计算
    2、多线城 ：I/O密集(网络I/O、本地磁盘I/O)
  2、多线程爬虫
    1、URL队列 ：put(url)
    2、RES队列 ：从URL队列中get()发请求,put(html)
    3、创建多个RES线程,发请求获取html
    4、创建多个解析线程,解析html
2、BeautifulSoup ：HTML/XML
  1、使用流程
    1、导入模块 ：from bs4 import BeautifulSoup
    2、创建对象 ：soup = BeautifulSoup(html,'lxml')
    3、查找节点 ：r_list = soup.find_all("",attrs={})
  2、支持解析库
    1、lxml
    2、html.parser
    3、xml
  3、常用方法
    1、find_all() : 列表
      rList = soup.find_all("div",attrs={"id":""})
    2、节点对象方法
      1、get_text()
      2、string
3、Scrapy框架
  1、异步处理框架
  2、组成
    Engine、Scheduler、Downloader、Item Pipeline、Spider、Downloader Middlewares、Spider Middlewares
  3、运行流程
    1、引擎开始统揽全局，向Spider索要URL
    2、引擎拿到URL后,给 调度器 入队列
    3、调度器从队列中拿出1个URL给引擎,通过 下载器中间件交给下载器去下载
    4、下载器下载完成,把响应交给引擎
    5、引擎把响应通过蜘蛛中间件交给爬虫程序
    6、spider处理完成后
      把数据通过引擎交给项目管道去处理
      把新的URL通过引擎再交给调度器去入队列
    7、调度器中没有任何的请求,程序结束
4、创建项目流程
  1、scrapy startproject Lianjia
  2、cd Lianjia/Lianjia
  3、定义爬取数据结构 ：subl items.py
    import scrapy
    class LianjiaItem(scrapy.Item):
        name = scrapy.Field()
	price = scrapy.Field()
  4、cd spiders
  5、scrapy genspider lianjia "lianjia.com"
    class LianjiaSpider(scrapy.Spider):
        name = "lianjia"
	allowed_domains = ["lianjia.com"]
	start_urls = [""]
	def parse(self,response):
	    ... ...
  6、项目管道 ：subl pipelines.py
    class LianjiaPipeline(object):
        def process_item(self,item,spider):
	    处理数据代码
  7、全局配置 ：subl settings.py
    ROBOTSTXT_OBEY = False
    USER_AGENT =
    ITME_PIPELINES = {"Lianjia.pipelines.LianjiaPipeline":200}
    DEAULT_REQUEST_HEADERS = {"":""}
  8、cd spiders
  9、scrapy crawl lianjia
5、driver如何执行js脚本
  driver.execute_script('window.scrollTo(0,document.baody.scrollHeight')
********************************************
Day07笔记
1、生成器(yield)
  1、yield作用 ：把1个函数当做1个生成器来使用
  2、yield特点 ：让函数暂停,等待下一次调用
2、项目 ：Csdn
  1、知识点 ：yield、pipelines.py
  2、网址：https://blog.csdn.net/zhuimengshaonian66/article/details/84473444
  3、目标
    标题 ： //h1[@class="title-article"]/text()
    发表时间 ： //span[@class="time"]/text()
    阅读数 ： //span[@class="read-count"]/text()
  4、步骤
    1、创建项目
    2、定义爬取数据结构
    3、创建爬虫程序
    4、写管道文件
    5、全局配置文件
    6、运行爬虫
3、知识点
  1、extract() ：获取选择器对象中的文本内容
    response.xpath('') 得到的结果为选择器对象的列表
  2、pipelines.py中必须有1个函数叫：
    def process_item(self,item,spider):
        return item
  3、爬虫程序中,start_urls必须为列表
4、项目 ：Daomu
  1、URL ：http://www.daomubiji.com/dao-mu-bi-ji-1
  2、爬取目标
    1、书的名称
      //h1[@class="focusbox-title"]/text()
    2、
      //article[@class="excerpt excerpt-c3"]/a/text()
      书的标题
      章节数量
      章节名称
    3、
      //article[@class="excerpt excerpt-c3"]/a/@href
      章节链接
5、MongoDB
  1、在settings.py中定义相关变量
    MONGODB_HOST =
    MONGODB_PORT =
    MONGODB_DATABASE =
    MONGODB_TABLE =
  2、在pipelines.py中添加项目管道
    class DaomuMongoPipeline(object):
        def __init__(self):
	    ... ...

	def process_item(self,item,spider):
	    ... ...
  3、在settings.py中设置项目管道
    ITEM_PIPELINES = {
         'Daomu.pipelines.DaomuMongoPipeline':200,
        }
6、MySQL使用同MongoDB
7、Scrapy设置log ：settings.py
  # 设置log级别
  LOG_LEVEL = "DEBUG"
  # 指定本地log文件
  LOG_FILE = "daomu.log"

  5层日志级别
    CRITICAL ：严重错误
    ERROR    ：一般错误
    WARNING  ：警告信息
    DEBUG    ：调试信息
    INFO     ：一般信息
8、腾讯招聘网站案例
  1、URL
    https://hr.tencent.com/position.php?start=0
    https://hr.tencent.com/position.php?start=10
  2、Xpath匹配
    //tr[@class="odd"] | //tr[@class="even"]
    职位名称 : ./td[1]/a/text()
    详情链接 : ./td[1]/a/@href
    职位类别 : ./td[2]/text()
    招聘人数 : ./td[3]/text()
    工作地点 : ./td[4]/text()
    发布时间 : ./td[5]/text()
9、Fiddler抓包工具使用
  1、抓包工具设置
    1、Tools->Options->HTTPS-> ...from browsers...
       Actions->Trust root Certificate
    2、Tools->Options->connections : 设置端口:8888
    3、重启Fiddler抓包工具
  2、设置浏览器代理
    1、Proxy SwitchOmega -> 选项 -> 新建情景模式 -> HTTP 127.0.0.1 8888 -> 左下角 应用选项
    2、点击浏览器右上角图标 -> AID1807 ->上网
  3、Fiddler常用选项
    1、选项卡 ：Inspectors
       Headers : 请求报头
       WebForms: POST请求表单数据(在body中)
       Raw     : 以文本形式显示整个请求信息
10、设置手机抓包
  1、Fiddler
  2、在手机上安装证书
    1、打开手机浏览器：http://IP地址:8888
       ## IP地址为电脑的IP地址(ipconfig)
    2、在页面上下载(FiddlerRoot certificate)
       下载文件名 ：FiddlerRoot.cer
    3、安装
  3、设置手机代理
    1、打开手机已连接的无线,代理设置->改成手动
    2、输入IP地址 ：你电脑的IP
       输入端口号 ：8888




















